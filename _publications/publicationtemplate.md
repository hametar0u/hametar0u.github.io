<!-- ---
title: "KaoGPT"
collection: publications
permalink: /publication/2023-03-22-KaoGPT
excerpt: 'This paper was written as the final project of ECE C147. This paper explores different architectures for text generation mimicating the tone of professor Jonathan Kao'
date: 2023-03-22
venue: 'Journal 1'
paperurl: 'http://hametar0u.github.io/files/KaoGPT.pdf'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
### Abstract
We developed text-generation models, including the RNN, decoder stack, encoder-decoder, and fine-tuned GPT- 2, to emulate Professor Kaoâ€™s lectures. Through experi- mentation, we found that finetuning GPT-2 led to a model that outperformed all others. However, given the lim- ited dataset, the trained-from-scratch decoder stack per- formed surprisingly well. Our results offer insights into the strengths and limitations of various text generation models, aiding researchers in selecting the most suitable model for their needs. -->