---
title: "KaoGPT: Studying the Performance of Text Generating Models"
excerpt: "This paper was written for the final project of ECE C147 Winter 2023. This paper explores different architectures for text generation mimicating the tone of professor Jonathan Kao"
collection: portfolio
paperurl: 'http://hametar0u.github.io/files/KaoGPT.pdf'
---

### Abstract
We developed text-generation models, including the RNN, decoder stack, encoder-decoder, and fine-tuned GPT- 2, to emulate Professor Kaoâ€™s lectures. Through experi- mentation, we found that finetuning GPT-2 led to a model that outperformed all others. However, given the lim- ited dataset, the trained-from-scratch decoder stack per- formed surprisingly well. Our results offer insights into the strengths and limitations of various text generation models, aiding researchers in selecting the most suitable model for their needs.
[View paper here](http://hametar0u.github.io/files/KaoGPT.pdf)